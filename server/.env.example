# =============================================================================
# Ollama on AWS EC2 (or any remote server)
# Set these so the backend always calls your Ollama instance — never localhost.
# =============================================================================

# EC2 (or VPS) URL where Ollama is running (systemctl-managed on Ubuntu).
# Example: http://3.12.45.67:11434  (use your EC2 public IP)
OLLAMA_HOST=http://YOUR_EC2_PUBLIC_IP:11434

# Model to use for chat, plan, insights (must be pulled on EC2: ollama pull <model>).
# Examples: llama3, qwen2.5:0.5b, mistral
OLLAMA_MODEL=llama3

# Optional: backup model if primary fails (e.g. smaller/faster model).
OLLAMA_BACKUP_MODEL=llama3.2

# Optional: for semantic search embeddings (only if you use that feature on EC2).
# OLLAMA_EMBED_MODEL=nomic-embed-text

# =============================================================================
# Other
# =============================================================================

# Optional: Gemini API key for image → text (free tier). Leave empty to disable.
GEMINI_API_KEY=

# Backend port (only this port is public; do not expose 11434 to the internet).
PORT=3001
